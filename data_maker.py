# -*- coding: utf-8 -*-
"""LSTMPrefetcher.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EvVoJKmwsI30VQz3jE4uYAOpwWj6O5O5
"""

# from google.colab import drive
# drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from nltk import ngrams
from sklearn.preprocessing import MinMaxScaler
from datetime import datetime
import tensorflow.keras.backend as K
from keras.utils import to_categorical
from collections import Counter

# ROOT_DIR = "/content/drive/My Drive/"
ROOT_DIR = "./"

def get_memory_addr_line(line):
	addr = 0
	rw = ""
	ip = 0
	tokens = line.split()
	if len(tokens)==3:
		try:
			addr = int(tokens[2], 16)
			rw = tokens[1]
			ip = int(tokens[0].strip(":"), 16)
		except:
			print("Error parsing...")
			return 0, "", 0
	return addr, rw, ip


def parse_data(FINAL_DATA_FILE = "data_processed.csv", final_data = {"delta": [], "rw": [], "ip": [], "addr": []}):
	with open(ROOT_DIR + "pinatrace_micro1.out", "r") as data_file:
		last_addr = 0
		lines = data_file.readlines()
		for line in lines:
			addr, rw, ip = get_memory_addr_line(line)
			if addr!=0 and rw == 'R':
				final_data["delta"].append(addr - last_addr)
				final_data["rw"].append(rw)
				final_data["ip"].append(ip)
				final_data["addr"].append(addr)
				last_addr = addr

	df = pd.DataFrame(final_data)
	df.to_csv(ROOT_DIR + FINAL_DATA_FILE, index=False, sep='\t')

from sklearn.cluster import KMeans

def get_and_analyze_data(DATA_FILE="data_processed.csv", start_perc=0.1, max_cnt=500, plot=False):
	data = pd.read_csv(ROOT_DIR + DATA_FILE, sep='\t')
	start_pos = int(data.shape[0] * (start_perc))
	data1 = data[start_pos:]
	print("Unique PCs: ", data["ip"].nunique())
	print("Unique Deltas: ", data["delta"].nunique())

	if plot:
		start_pos = int(data.shape[0] * (start_perc))
		y = data["delta"].astype(float).values[start_pos:start_pos+max_cnt]
		yaddr = data["addr"].astype(float).values[start_pos:start_pos+max_cnt]
		x = np.array([i for i in range(len(y))])

		yaddr = yaddr.reshape(-1, 1)
		kmeans = KMeans(n_clusters=2, random_state=0).fit(yaddr)
		yaddr = yaddr.reshape(-1)

		first = yaddr[kmeans.labels_==0]
		second = yaddr[kmeans.labels_==1]

		firstx = x[kmeans.labels_==0]
		secondx = x[kmeans.labels_==1]

		plt.figure(1)
		plt.title("Address over time")
		plt.xlabel("Index")
		plt.ylabel("Address")
		plt.plot(firstx, first, 'r.')
		plt.plot(secondx, second, 'b.')
		plt.show()

		plt.figure(2)
		plt.title("Delta over time")
		plt.xlabel("Index")
		plt.ylabel("Delta")
		plt.plot(x, y, 'k.')
		plt.show()

	return data1


def create_dataset(data, maxlen=10, data_cnt=50000, num_classes=3000):

	scaler = MinMaxScaler((0, 1))

	deltas = data["delta"].astype(float).values[:data_cnt*2]
	ips = data["ip"].astype(float).values[:data_cnt*2]
	addrs = data["addr"].astype(float).values[:data_cnt*2]

	del_freq = Counter(deltas)
	max10000 = del_freq.most_common(num_classes)
	del_list = {}
	total_cnt = 0.
	for idx, delta in enumerate(max10000):
		del_list[delta[0]] = idx
		total_cnt += delta[1]

	print("Proportion of data in top {}: {}".format(num_classes, total_cnt / len(deltas)))

	ips = ips.reshape(-1, 1)
	ips = scaler.fit_transform(ips)
	ips = ips.reshape(-1)

	addrs = addrs.reshape(-1, 1)
	kmeans = KMeans(n_clusters=2, random_state=0).fit(addrs)
	addrs = addrs.reshape(-1)

	cluster_ids = kmeans.labels_[:]

	firstclass = addrs[cluster_ids==0]
	secondclass = addrs[cluster_ids==1]

	firstclass = firstclass.reshape(-1, 1)
	secondclass = secondclass.reshape(-1, 1)

	firstclass = scaler.fit_transform(firstclass)
	secondclass = scaler.fit_transform(secondclass)

	firstclass = firstclass.reshape(-1)
	secondclass = secondclass.reshape(-1)

	addrs[cluster_ids==0] = firstclass
	addrs[cluster_ids==1] = secondclass

	ng1 = ngrams(ips, maxlen+1)
	ng2 = ngrams(cluster_ids, maxlen+1)
	ng3 = ngrams(addrs, maxlen+1)
	ng4 = ngrams(deltas, maxlen+1)

	ng1 = [ngg for ngg in ng1]
	ng2 = [ngg for ngg in ng2]
	ng3 = [ngg for ngg in ng3]
	ng4 = [ngg for ngg in ng4]
	inds = np.random.choice([i for i in range(len(ng1))], data_cnt, replace=False)
	
	ng1 = np.array(ng1)[inds]
	ng2 = np.array(ng2)[inds]
	ng3 = np.array(ng3)[inds]
	ng4 = np.array(ng4)[inds]

	X = []
	y = []
	for indx, _ in enumerate(ng1):
		if ng4[indx][-1] in del_list:
			X.append(list(zip(ng1[indx][:-1], ng2[indx][:-1], ng3[indx][:-1])))
			y.append(del_list[ng4[indx][-1]])

	y = to_categorical(y, num_classes=num_classes)
	return np.array(X).reshape(-1, maxlen, 3), np.array(y)

parse_data()
data = get_and_analyze_data(start_perc=0.003, plot=True)

# import tensorflow as tf
# import os
# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
# tf.config.experimental_connect_to_cluster(resolver)
# tf.tpu.experimental.initialize_tpu_system(resolver)
# strategy = tf.distribute.experimental.TPUStrategy(resolver)

from sklearn.model_selection import train_test_split

class MyCustomCallback(tf.keras.callbacks.Callback):
  def __init__(self, test_data, k=10):
    self.X_test, self.y_test = test_data
    self.k = k

  def on_train_batch_end(self, batch, logs=None):
    if batch%1000!=0:
      return
    y_test_pred = self.model.predict(self.X_test)
    topk = y_test_pred.argsort()[:, -self.k:]
    acc = 0.
    for i in range(self.y_test.shape[0]):
      if np.argmax(self.y_test[i]) in topk[i]:
        acc += 1.
    print("ACC: ", acc / float(self.y_test.shape[0]))
    

def get_lstm_model(X_train, y_train, X_test, y_test, rnn_units=32, batch_size=64, maxlen=10, num_labels=3, num_classes=3000, epochs=3, toplot=False):
  input_layer1 = tf.keras.layers.Input(shape=(maxlen, 3, ), name='input1')

  #LSTM Layer 1 with Dropout
  lstm_layer1 = tf.keras.layers.LSTM(rnn_units, return_sequences=True)(input_layer1)
  lstm_layer1 = tf.keras.layers.Dropout(0.1)(lstm_layer1)

  #LSTM Layer 2 with Dropout
  lstm_layer2 = tf.keras.layers.LSTM(rnn_units)(lstm_layer1)
  penultimate = tf.keras.layers.Dropout(0.1)(lstm_layer2)

  #Output Layer
  prediction = tf.keras.layers.Dense(num_classes, activation='softmax')(penultimate)

  #Model Inputs and outputs
  model = tf.keras.models.Model(inputs=input_layer1, outputs=prediction)
  #Use Categorical Loss and Accuracy with Adam Optimizer
  model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])
  print(model.summary())

  #Fit the model to the data
  history = model.fit(X, y, batch_size=batch_size, epochs=epochs, shuffle=False, callbacks=[MyCustomCallback((X_test, y_test))])

  #Plot the loss curves
  if toplot:
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'val'], loc='upper left')
    plt.show()

  return model

X, y = create_dataset(data, data_cnt=40000, num_classes=5000, maxlen=5)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1234)

print("Training X & y: ", X_train.shape, y_train.shape)
print("Validation X & y: ", X_test.shape, y_test.shape)

#Use TPU Strategy
# with strategy.scope():
model = get_lstm_model(X_train, y_train, X_test, y_test, rnn_units=512, batch_size=8, num_classes=5000, maxlen=5, epochs=10, toplot=True)

